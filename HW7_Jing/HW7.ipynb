{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back\n",
      "[Synset('back.n.01'), Synset('rear.n.05'), Synset('back.n.03'), Synset('back.n.04'), Synset('spinal_column.n.01'), Synset('binding.n.05'), Synset('back.n.07'), Synset('back.n.08'), Synset('back.n.09'), Synset('back.v.01'), Synset('back.v.02'), Synset('second.v.01'), Synset('back.v.04'), Synset('back.v.05'), Synset('back.v.06'), Synset('bet_on.v.01'), Synset('back.v.08'), Synset('back.v.09'), Synset('back.v.10'), Synset('back.a.01'), Synset('back.s.02'), Synset('back.s.03'), Synset('back.r.01'), Synset('back.r.02'), Synset('back.r.03'), Synset('back.r.04'), Synset('back.r.05'), Synset('back.r.06')]\n",
      "Clean\n",
      "[Synset('clean_and_jerk.n.01'), Synset('clean.v.01'), Synset('clean.v.02'), Synset('houseclean.v.01'), Synset('cleanse.v.01'), Synset('clean.v.05'), Synset('clean.v.06'), Synset('clean.v.07'), Synset('clean.v.08'), Synset('scavenge.v.04'), Synset('clean.v.10'), Synset('clean.a.01'), Synset('clean.s.02'), Synset('clean.s.03'), Synset('clean.s.04'), Synset('clean.s.05'), Synset('clean.a.06'), Synset('clean.a.07'), Synset('clean.a.08'), Synset('uninfected.s.01'), Synset('clean.s.10'), Synset('clean.s.11'), Synset('blank.s.01'), Synset('clean.s.13'), Synset('clean.s.14'), Synset('clean.s.15'), Synset('clean.s.16'), Synset('clean.s.17'), Synset('clean.s.18'), Synset('clean.r.01'), Synset('fairly.r.03')]\n",
      "Clear\n",
      "[Synset('clear.n.01'), Synset('open.n.01'), Synset('unclutter.v.01'), Synset('clear.v.02'), Synset('clear_up.v.04'), Synset('authorize.v.01'), Synset('clear.v.05'), Synset('pass.v.09'), Synset('clear.v.07'), Synset('clear.v.08'), Synset('clear.v.09'), Synset('clear.v.10'), Synset('clear.v.11'), Synset('clear.v.12'), Synset('net.v.02'), Synset('net.v.01'), Synset('gain.v.08'), Synset('clear.v.16'), Synset('clear.v.17'), Synset('acquit.v.01'), Synset('clear.v.19'), Synset('clear.v.20'), Synset('clear.v.21'), Synset('clear.v.22'), Synset('clear.v.23'), Synset('clear.v.24'), Synset('clear.a.01'), Synset('clear.s.02'), Synset('clear.s.03'), Synset('clear.a.04'), Synset('clear.s.05'), Synset('clear.s.06'), Synset('clean.s.03'), Synset('clear.s.08'), Synset('clear.s.09'), Synset('well-defined.a.02'), Synset('clear.a.11'), Synset('clean.s.02'), Synset('clear.s.13'), Synset('clear.s.14'), Synset('clear.s.15'), Synset('absolved.s.01'), Synset('clear.s.17'), Synset('clear.r.01'), Synset('clearly.r.04')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "words = ['Back','Clean','Clear']\n",
    "for i in words:\n",
    "    print(i)\n",
    "    print(wn.synsets(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'NP', 'VP']\n",
      "['S', 'S0', 'VP']\n",
      "['S0', 'Aux', 'NP']\n",
      "['NP', 'Det', 'Nominal']\n",
      "['Nominal', 'Nominal', 'Noun']\n",
      "['Nominal', 'Nominal', 'PP']\n",
      "['VP', 'Verb', 'NP']\n",
      "['VP', 'VP1', 'PP']\n",
      "['VP1', 'Verb', 'NP']\n",
      "['VP', 'Verb', 'PP']\n",
      "['VP', 'VP', 'PP']\n",
      "['PP', 'Preposition', 'NP']\n",
      "['Det', 'Det4', 'Det2']\n",
      "Det2\n",
      "'that'\n",
      "Det2\n",
      "'this'\n",
      "Det2\n",
      "'the'\n",
      "Det2\n",
      "'a'\n",
      "['Det3', 'Det2', 'Det2']\n",
      "['Det4', 'Det3', 'Det2']\n",
      "['Noun', 'Noun7', 'Noun5']\n",
      "Noun5\n",
      "'book'\n",
      "Noun5\n",
      "'flight'\n",
      "Noun5\n",
      "'meal'\n",
      "Noun5\n",
      "'money'\n",
      "['Noun6', 'Noun5', 'Noun5']\n",
      "['Noun7', 'Noun6', 'Noun5']\n",
      "['Verb', 'Verb9', 'Verb8']\n",
      "Verb8\n",
      "'book'\n",
      "Verb8\n",
      "'include'\n",
      "Verb8\n",
      "'prefer'\n",
      "['Verb9', 'Verb8', 'Verb8']\n",
      "['Pronoun', 'Pronoun11', 'Pronoun10']\n",
      "Pronoun10\n",
      "'I'\n",
      "Pronoun10\n",
      "'she'\n",
      "Pronoun10\n",
      "'me'\n",
      "['Pronoun11', 'Pronoun10', 'Pronoun10']\n",
      "['ProperNoun', 'ProperNoun12', 'ProperNoun12']\n",
      "ProperNoun12\n",
      "'Houston'\n",
      "ProperNoun12\n",
      "'NWA'\n",
      "['Aux', \"'does'\"]\n",
      "['Preposition', 'Preposition16', 'Preposition13']\n",
      "Preposition13\n",
      "'from'\n",
      "Preposition13\n",
      "'to'\n",
      "Preposition13\n",
      "'on'\n",
      "Preposition13\n",
      "'near'\n",
      "Preposition13\n",
      "'through'\n",
      "['Preposition14', 'Preposition13', 'Preposition13']\n",
      "['Preposition15', 'Preposition14', 'Preposition13']\n",
      "['Preposition16', 'Preposition15', 'Preposition13']\n",
      "['VP', 'Verb9', 'Verb8']\n",
      "['VP', 'Verb9', 'Verb8']\n",
      "['VP', 'Verb9', 'Verb8']\n",
      "['Nominal', 'Noun7', 'Noun5']\n",
      "['Nominal', 'Noun7', 'Noun5']\n",
      "['Nominal', 'Noun7', 'Noun5']\n",
      "['NP', 'ProperNoun12', 'ProperNoun12']\n",
      "['NP', 'ProperNoun12', 'ProperNoun12']\n",
      "['NP', 'ProperNoun12', 'ProperNoun12']\n",
      "['NP', 'Pronoun11', 'Pronoun10']\n",
      "['NP', 'Pronoun11', 'Pronoun10']\n",
      "['NP', 'Pronoun11', 'Pronoun10']\n",
      "['S', 'Verb', 'NP']\n",
      "['S', 'VP1', 'PP']\n",
      "['S', 'Verb', 'PP']\n",
      "['S', 'VP', 'PP']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb', 'NP']\n",
      "['S', 'VP1', 'PP']\n",
      "['S', 'Verb', 'PP']\n",
      "['S', 'VP', 'PP']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb', 'NP']\n",
      "['S', 'VP1', 'PP']\n",
      "['S', 'Verb', 'PP']\n",
      "['S', 'VP', 'PP']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n",
      "['S', 'Verb9', 'Verb8']\n"
     ]
    }
   ],
   "source": [
    "from grammar_converter import convert_grammar, read_grammar\n",
    "grammer = read_grammar('grammer.txt')\n",
    "cnf = convert_grammar(grammer)\n",
    "for i in cnf:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SBAR+S\n",
      "  (NP (PRP I))\n",
      "  (VP\n",
      "    (VBN shot)\n",
      "    (NP (DT an) (NN elephant))\n",
      "    (PP (IN in) (NP (PRP$ my) (NN pajamas)))))\n",
      "(S+NP\n",
      "  (NN book)\n",
      "  (NP (DT the) (NN cooks))\n",
      "  (SBAR (WHNP (WP who)) (S+VP (VBZ cook) (NP (DT the) (NNS books)))))\n"
     ]
    }
   ],
   "source": [
    "from stat_parser import Parser, display_tree\n",
    "\n",
    "parser = Parser()\n",
    "\n",
    "tree = parser.parse(\"I shot an elephant in my pajamas\")\n",
    "tree2 = parser.parse(\"Book the cooks who cook the books\")\n",
    "\n",
    "print(tree)\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'the', 'cooks', 'who', 'cook', 'the', 'books']\n",
      "(S\n",
      "  (VP (TV Book) (NP (Det the (N cooks)) (N who (TV cook))))\n",
      "  (NP (Det the) (N books)))\n",
      "(S\n",
      "  (VP (TV Book) (NP (Det the) (N cooks)))\n",
      "  (NP (N who (TV cook)) (NP (Det the) (N books))))\n"
     ]
    }
   ],
   "source": [
    "from nltk import Nonterminal, nonterminals, Production, CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "groucho_grammar = CFG.fromstring('''\n",
    "S -> VP NP\n",
    "VP -> TV NP\n",
    "TV -> 'Book'\n",
    "NP -> Det N\n",
    "Det -> 'the' N\n",
    "N -> 'cooks'\n",
    "NP -> N NP\n",
    "N -> 'who' TV\n",
    "TV -> 'cook'\n",
    "NP -> Det N\n",
    "Det -> 'the'\n",
    "N -> 'books'\n",
    "  ''')\n",
    "sent = word_tokenize(\"Book the cooks who cook the books\")\n",
    "print(sent)\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "for tree in parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S+NP\n",
      "  (NN book)\n",
      "  (NP (DT the) (NN cooks))\n",
      "  (SBAR (WHNP (WP who)) (S+VP (VBZ cook) (NP (DT the) (NNS books)))))\n"
     ]
    }
   ],
   "source": [
    "from stat_parser import Parser, display_tree\n",
    "\n",
    "parser = Parser()\n",
    "tree = parser.parse(\"Book the cooks who cook the books\")\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345\n",
      "1370\n",
      "764\n",
      "4347\n",
      "935\n",
      "532\n",
      "1405\n",
      "369\n",
      "446\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "booklist = [text1,text2,text3,text4,text5,text6,text7,text8,text9]\n",
    "longest = []\n",
    "\n",
    "for i in booklist:\n",
    "    text_to = TreebankWordDetokenizer().detokenize(i)\n",
    "    longest.append(max(sent_tokenize(text_to), key=len))\n",
    "\n",
    "for i in longest:\n",
    "    print(len(i))\n",
    "\n",
    "longestsent = len(max(longest, key=len)), max(longest, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 35 yr old OUTGOING M seeks fem 28 - 35 for o / door sports - w / e away A professional business male, late 40s , 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship .\n"
     ]
    }
   ],
   "source": [
    "print(longest[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FRAG\n",
      "  (FRAG\n",
      "    (FRAG\n",
      "      (NP (QP (CD 2) (CD 35)) (NNS yr))\n",
      "      (NP (JJ old) (NN OUTGOING)))\n",
      "    (NP (JJ M) (NN seeks))\n",
      "    (NP (JJ fem) (CD 28)))\n",
      "  (: -)\n",
      "  (NP\n",
      "    (NP (CD 35))\n",
      "    (PP (IN for) (NP (JJ o) (CD /) (NN door) (NNS sports))))\n",
      "  (: -)\n",
      "  (S\n",
      "    (NP (JJ w) (CD /) (NNS e))\n",
      "    (ADVP (RB away))\n",
      "    (NP (DT A) (JJ professional) (NN business) (NN male))\n",
      "    (, ,)\n",
      "    (NP (JJ late) (NN 40s))\n",
      "    (, ,)\n",
      "    (NP (CD 6) (NNS feet))\n",
      "    (NP (JJ tall) (, ,) (NP (NNP slim)))\n",
      "    (VP\n",
      "      (VB build)\n",
      "      (UCP\n",
      "        (, ,)\n",
      "        (ADJP (RB well) (VBN groomed))\n",
      "        (, ,)\n",
      "        (JJ great)\n",
      "        (NN personality)\n",
      "        (, ,)\n",
      "        (NN home)\n",
      "        (NN owner)\n",
      "        (, ,)\n",
      "        (NNS interests)\n",
      "        (VP\n",
      "          (VBP include)\n",
      "          (SBAR+S\n",
      "            (NP (DT the) (NN arts))\n",
      "            (VP (VB travel) (NP (CC and) (DT all) (NNS things)))))\n",
      "        (ADJP (JJ good))\n",
      "        (, ,)\n",
      "        (NNP Ringwood)\n",
      "        (NN area)\n",
      "        (, ,)\n",
      "        (VP (VBZ is) (VBG seeking) (NP (DT a) (NN genuine)))\n",
      "        (ADJP (JJ female) (PP (IN of) (NP (JJ similar) (NN age))))\n",
      "        (CC or)\n",
      "        (ADJP (JJR older))\n",
      "        (, ,)\n",
      "        (PP (IN in) (NP (JJ same) (NN area)))\n",
      "        (CC or)\n",
      "        (NN surrounds)\n",
      "        (, ,)\n",
      "        (PP (IN for) (NP (DT a) (NN meaningful)))\n",
      "        (JJ long)\n",
      "        (NN term)\n",
      "        (NNS rship))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "tree = parser.parse(longest[7])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stat_parser is based on Penn treebanks, while the nltk is based on writen grammer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
